# # Llama3 w/ vLLM
# # Python

from vllm import LLM, SamplingParams

from huggingface_hub import notebook_login

import transformers
import sys


from flask import Flask, request, render_template
from flask.json import jsonify

from huggingface_hub import login
from pymilvus.model.hybrid import BGEM3EmbeddingFunction

from pymilvus import connections, Collection, utility, FieldSchema, DataType, CollectionSchema

app = Flask(__name__)

# Set the Flask environment to development
app.config['ENV'] = 'development'
app.config['DEBUG'] = True

# "/" è la home page
@app.route("/")
def home():
    return render_template("agriveritas.html")

import datetime


llm = None
bge_m3_ef = None

@app.route("/initInfo")
def get_init_info():
    current_date = datetime.date.today()
    print(type(current_date))
    last_update_date = current_date.strftime("%Y/%m/%d")

    return jsonify({"last_update_date": last_update_date})


@app.route("/IAresponse/<regione>")
def IAResponse(regione="noRegionSelected"):
    response_status = 0
    
    # Get the 'qt' query parameter
    query_text = request.args.get('qt')
    
    # If the 'qt' parameter is not provided, use the default query
    if not query_text:
        query_text = "Cosa deve fare un Imprenditore Agricolo?"
    
    response_text = manageSmartResponse(query_text)

    if(regione == "noRegionSelected"):
        response_status = 1
        response_text = "Seleziona prima una regione, altrimenti non riesco ad aiutarti al meglio!"
    
    response_data = {"status" : response_status, "message" : response_text}

    # current_date = datetime.date.today()
    # print(type(current_date))

    return jsonify(response_data)





import time
from huggingface_hub import login, snapshot_download
from pymilvus.model.hybrid import BGEM3EmbeddingFunction

from pymilvus import connections, Collection, utility, FieldSchema, DataType, CollectionSchema

import random
from itertools import chain

def generateRandomEmbedding(amount):
    return [random.uniform(0.0, 1.0) for _ in range(amount)]


snapshot_download(repo_id='BAAI/bge-m3',local_dir="bge-m3-onnx")

bge_m3_ef = BGEM3EmbeddingFunction(
    model_name='BAAI/bge-m3',
    device='cuda',
    use_fp16=True
)


from langchain_community.llms import VLLM
from transformers import AutoTokenizer
import os

HF_TOKEN = "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"
model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
login(token=HF_TOKEN)
tokenizer = AutoTokenizer.from_pretrained(model_id)
terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

# Decrease gpu_memory_utilization
gpu_memory_utilization = 0.7  # Try a lower value, e.g., 0.6

# Enforce eager mode
os.environ["TF_ENABLE_EAGER_EXECUTION"] = "1"

# Reduce max_num_seqs
max_num_seqs = 16  # Try a lower value, e.g., 8

llm = VLLM(
    model=model_id,
    trust_remote_code=True,  # mandatory for hf models
    max_new_tokens=512,
    temperature=0.9,
    gpu_memory_utilization=gpu_memory_utilization,
    top_p=1.0,
    top_k=10,
    download_dir="./models",
    use_beam_search=False,
    dtype="auto",
    stop=terminators,
    vllm_kwargs={"enforce_eager": True}, #, "quantization": "awq", "max_model_len": 4096
)


def manageSmartResponse(query):
    # # # # https://github.com/LazaUK/AOAI-LangChain-Milvus/blob/main/AOAI_langchain_Milvus.ipynb
    
    connections.connect("default", host="milvus-standalone", port="19530")   #host="milvus-standalone"

    # Load the existing collections
    risorsa_collection = Collection("RISORSA")
    chunk_collection = Collection("CHUNK")

    # # Load the collection
    risorsa_collection.load()
    chunk_collection.load()

    query_text = "Cosa deve fare un Imprenditore Agricolo?"

    embedded = bge_m3_ef.encode_documents([query_text])['dense'][0]

    searchingChunk = embedded.tolist() # generateRandomEmbedding(1024)
    
    # print("--> " + str(searchingChunk))
    # print(f"Vector dimension of searchingChunk: {len(searchingChunk)}")
    # print(f"Vector dimension of embedded: {embedded.shape}")
    # collection_info = chunk_collection.describe()
    #print(f"Vector dimension of the 'embedding' field: {collection_info.fields['embedding'].dim}")

    results = chunk_collection.search(
        data = [searchingChunk], 
        anns_field = "embedding", 
        limit=2, 
        param={
            "metric_type": "L2", 
            "params": {
                "nprobe": 10,
                },
            "topk": 5
            },
        output_fields=["ID_univoco_risorsa", "ID_counter", "relative_text"]
        )

    risposta = ""

    print("\n\n\n")
    print("Search Results:")
    print(type(results))
    for hits in results: # GUARDA https://milvus.io/api-reference/pymilvus/v2.4.x/ORM/Collection/search.md
        for index, hit in enumerate(hits):
            print(type(hit))
            print(f"ID: {hit.id}") #.entity['ID_univoco_risorsa']} - {result.entity['ID_counter']}")
            print(f"Distance {hit.score}") 
            # print(f"Vector {hit.vector}") 
            print(f"Relative Text: {hit.get('relative_text')}")
            print("-" * 80)
            risposta = risposta + str(index) + ") " + hit.get('relative_text') + "\n"

    print("\n\n\n")


    # # # Disconnect from the Milvus service
    connections.disconnect("default")

    domanda = query_text #"METTI QUI LA DOMANDA"

    regione = "Emilia Romagna"
    initLog = f"""Sei un esperto funzionario della AgEA dell' {regione} e il tuo compito è assistere delle persone che ti pongono delle domande
    Di seguito quindi avrai un quesito e delle possibili risposte che possono risolvere il dubbio
    Domanda: {domanda}
    Possibili risposte: 
    {risposta}
    È di fondamentale importanza che tu risponda in italiano e usando il più possibile il lessico tecnico agronomico
    """

    print(initLog)


    query = initLog
    # input_ids = tokenizer.encode([query], return_tensors="pt")
    result = llm.generate(prompts=[query])
    print(result)


    connections.connect("default", host="milvus-standalone", port="19530")   #host="milvus-standalone"
    
    embedded = bge_m3_ef.encode_documents([query])['dense'][0]
    print(len(embedded))
    for a in embedded:
        print(a)
    
    # Load the existing collections
    risorsa_collection = Collection("RISORSA")
    chunk_collection = Collection("CHUNK")

    # # Load the collection
    risorsa_collection.load()
    chunk_collection.load()

    results = chunk_collection.search(
        data = [embedded], 
        anns_field = "embedding", 
        limit=3, 
        param={
            "metric_type": "L2", 
            "params": {
                "nprobe": 10,
                },
            "topk": 5
            },
        output_fields=["ID_univoco_risorsa", "ID_counter", "relative_text"]
    )
    
    connections.disconnect("default")

    regione = "Emilia Romagna"
    domanda = query
    risposta = ""
    for hits in results: # GUARDA https://milvus.io/api-reference/pymilvus/v2.4.x/ORM/Collection/search.md
        for index, hit in enumerate(hits):
            print(type(hit))
            print(f"ID: {hit.id}") #.entity['ID_univoco_risorsa']} - {result.entity['ID_counter']}")
            print(f"Distance {hit.score}") 
            # print(f"Vector {hit.vector}") 
            print(f"Relative Text: {hit.get('relative_text')}")
            print("-" * 80)
            risposta = risposta + str(index) + ") " + hit.get('relative_text') + "\n"

    initLog = f"""Sei un esperto funzionario della AgEA dell' {regione} e il tuo compito è assistere delle persone che ti pongono delle domande
            Di seguito quindi avrai un quesito e delle possibili risposte che possono risolvere il dubbio
            Domanda: {domanda}
            Possibili risposte: 
            {risposta}
            È di fondamentale importanza che tu risponda in italiano e usando il più possibile il lessico tecnico agronomico
            """


    return initLog
    # sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
    
    # prompts = [
    #     query
    # ]
    # prompt_template="[INST] {prompt} [/INST]"
    # prompts = [prompt_template.format(prompt=prompt) for prompt in prompts]
    # outputs = llm.generate(prompts, sampling_params)
    # # Print the outputs.
    # for output in outputs:
    #     prompt = output.prompt
    #     generated_text = output.outputs[0].text
    #     print(f"Prompt: {prompt!r}, Generated text: {generated_text}")
    
    # return outputs[0].outputs[0].text

# import os
# # Create the model download directory
# model_dir = "./models"
# os.makedirs(model_dir, exist_ok=True)

# # Set the TRANSFORMERS_CACHE environment variable
# os.environ["TRANSFORMERS_CACHE"] = model_dir

# bge_m3_ef = BGEM3EmbeddingFunction(
#     model_name='BAAI/bge-m3',
#     device='cuda',
#     use_fp16=True
# )

'''
# from langchain_community.llms import VLLM
# from transformers import AutoTokenizer
# import os

# HF_TOKEN = "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"
# model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
# login(token=HF_TOKEN)
# tokenizer = AutoTokenizer.from_pretrained(model_id)
# terminators = [
#     tokenizer.eos_token_id,
#     tokenizer.convert_tokens_to_ids("<|eot_id|>")
# ]

# # Decrease gpu_memory_utilization
# gpu_memory_utilization = 0.7  # Try a lower value, e.g., 0.6

# # Enforce eager mode
# os.environ["TF_ENABLE_EAGER_EXECUTION"] = "1"

# # Reduce max_num_seqs
# max_num_seqs = 16  # Try a lower value, e.g., 8

# llm = VLLM(
#     model=model_id,
#     trust_remote_code=True,  # mandatory for hf models
#     max_new_tokens=512,
#     temperature=0.9,
#     gpu_memory_utilization=gpu_memory_utilization,
#     top_p=1.0,
#     top_k=10,
#     download_dir="./models",
#     use_beam_search=False,
#     dtype="auto",
#     stop=terminators,
#     vllm_kwargs={"enforce_eager": True}, #, "quantization": "awq", "max_model_len": 4096
# )
'''

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8501, debug=True)